{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPRVq3e03c1K"
   },
   "source": [
    "# File QA RAG Chatbot App with ChatGPT, LangChain and Chainlit\n",
    "\n",
    "Here we will implement an advanced RAG System with ChatGPT, LangChain and Chainlit to build a File QA UI-based chatbot with the following features:\n",
    "\n",
    "- PDF Document Upload and Indexing\n",
    "- RAG System for query analysis and response\n",
    "- Result streaming capabilities (Real-time output)\n",
    "- Show document sources of the answer from RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install App and LLM dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.11\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11\n",
    "!pip install chainlit==1.3.2\n",
    "!pip install pyngrok==7.2.2\n",
    "!pip install PyMuPDF==1.24.0 #to extract \n",
    "!pip install chromadb==0.5.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwGjVWK4q6F"
   },
   "source": [
    "## Load OpenAI API Credentials\n",
    "\n",
    "Here we load it from a file so we don't explore the credentials on the internet by mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e1HqI56y7t3"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryheOZuXxa41"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
    "    api_creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZs7ts6NzADJ",
    "outputId": "2ffb5161-9785-499f-ff1c-fbcceebc938b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['openai_key'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_creds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDe44J0N0NcC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCMshwB1U9iQ"
   },
   "source": [
    "## Write the app code here and store it in a py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXceMDF0Qza0",
    "outputId": "4878586f-ae9e-481a-89a1-45571e20f46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "# the following line is a magic command\n",
    "# that will write all the code below it to the python file app.py\n",
    "# we will then deploy this app.py file on the cloud server where colab is running\n",
    "# if you have your own server you can just write the code in app.py and deploy it directly\n",
    "%%writefile app.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from operator import itemgetter\n",
    "import chainlit as cl\n",
    "import tempfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Takes uploaded PDFs, creates document chunks, computes embeddings\n",
    "# Stores document chunks and embeddings in a Vector DB\n",
    "# Returns a retriever which can look up the Vector DB\n",
    "# to return documents based on user input\n",
    "def configure_retriever(uploaded_files):\n",
    "  # Read documents\n",
    "  docs = []\n",
    "  temp_dir = tempfile.TemporaryDirectory()\n",
    "  for file in uploaded_files:\n",
    "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "    with open(temp_filepath, \"wb\") as f:\n",
    "      with open(file.path, 'rb') as infile:\n",
    "        f.write(infile.read())\n",
    "    loader = PyMuPDFLoader(temp_filepath)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "  # Split into documents chunks\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                                 chunk_overlap=200)\n",
    "  doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "  # Create document embeddings and store in Vector DB\n",
    "  embeddings_model = OpenAIEmbeddings()\n",
    "  vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
    "\n",
    "  # Define retriever object\n",
    "  retriever = vectordb.as_retriever()\n",
    "  return retriever\n",
    "\n",
    "@cl.on_chat_start\n",
    "# this function is called when the app starts for the first time\n",
    "async def when_chat_starts():\n",
    "  # Create UI element to accept PDF uploads\n",
    "  uploaded_files = None\n",
    "  # Wait for the user to upload a file\n",
    "  while uploaded_files == None:\n",
    "    uploaded_files = await cl.AskFileMessage(\n",
    "      content=\"Please upload PDF documents to continue.\",\n",
    "      accept=[\"application/pdf\"],\n",
    "      max_size_mb=20, max_files=5, timeout=180\n",
    "    ).send()\n",
    "\n",
    "  msg = cl.Message(content=f\"Processing files please wait...\", disable_feedback=True)\n",
    "  await msg.send()\n",
    "  await cl.sleep(2)\n",
    "  # Create retriever object based on uploaded PDFs\n",
    "  retriever = configure_retriever(uploaded_files)\n",
    "  msg = cl.Message(content=f\"Processing completed. You can now ask questions!\", disable_feedback=True)\n",
    "  await msg.send()\n",
    "\n",
    "  # Load a connection to ChatGPT LLM\n",
    "  chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
    "                      streaming=True)\n",
    "\n",
    "  # Create a prompt template for QA RAG System\n",
    "  qa_template = \"\"\"\n",
    "                Use only the following pieces of context to answer the question at the end.\n",
    "                If you don't know the answer, just say that you don't know,\n",
    "                don't try to make up an answer. Keep the answer as concise as possible.\n",
    "\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "                \"\"\"\n",
    "  qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "  # This function formats retrieved documents before sending to LLM\n",
    "  def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "  # Create a QA RAG System Chain\n",
    "  qa_rag_chain = (\n",
    "    {\n",
    "      \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
    "        |\n",
    "      retriever\n",
    "        |\n",
    "      format_docs,\n",
    "      \"question\": itemgetter(\"question\") # user question\n",
    "    }\n",
    "      |\n",
    "    qa_prompt # prompt with above user question and context\n",
    "      |\n",
    "    chatgpt # above prompt is sent to the LLM for response\n",
    "      |\n",
    "    StrOutputParser() # to parse the output to show on UI\n",
    "  )\n",
    "  # Set session variables to be accessed when user enters prompts in the app\n",
    "  cl.user_session.set(\"qa_rag_chain\", qa_rag_chain)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "# this function is called whenever the user sends a prompt message in the app\n",
    "async def on_user_message(message: cl.Message):\n",
    "\n",
    "  # get the chain and memory objects from the session variables\n",
    "  qa_rag_chain = cl.user_session.get(\"qa_rag_chain\")\n",
    "\n",
    "  # this will store the response from ChatGPT LLM\n",
    "  chatgpt_message = cl.Message(content=\"\")\n",
    "\n",
    "  #Callback handler for handling the retriever and LLM processes.\n",
    "  # Used to post the sources of the retrieved documents as a Chainlit element.\n",
    "  class PostMessageHandler(BaseCallbackHandler):\n",
    "    def __init__(self, msg: cl.Message):\n",
    "      BaseCallbackHandler.__init__(self)\n",
    "      self.msg = msg\n",
    "      self.sources = []\n",
    "\n",
    "    def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
    "      source_ids = []\n",
    "      for d in documents: # retrieved documents from retriever based on user query\n",
    "        metadata = {\n",
    "          \"source\": d.metadata[\"source\"],\n",
    "          \"page\": d.metadata[\"page\"],\n",
    "          \"content\": d.page_content[:200]\n",
    "        }\n",
    "        idx = (metadata[\"source\"], metadata[\"page\"])\n",
    "        if idx not in source_ids: # store unique source documents\n",
    "          source_ids.append(idx)\n",
    "          self.sources.append(metadata)\n",
    "\n",
    "    def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
    "      if len(self.sources):\n",
    "          sources_table = pd.DataFrame(self.sources[:3]).to_markdown()\n",
    "          self.msg.elements.append(\n",
    "            cl.Text(name=\"Sources\", content=sources_table, display=\"inline\")\n",
    "          )\n",
    "\n",
    "  # Stream the response from ChatGPT and show in real-time\n",
    "  async with cl.Step(type=\"run\", name=\"QA Assistant\"):\n",
    "    async for chunk in qa_rag_chain.astream(\n",
    "        {\"question\": message.content},\n",
    "        config=RunnableConfig(callbacks=[\n",
    "            cl.LangchainCallbackHandler(),\n",
    "            PostMessageHandler(chatgpt_message)\n",
    "        ]),\n",
    "    ):\n",
    "        await chatgpt_message.stream_token(chunk)\n",
    "  await chatgpt_message.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8de1tM6FVLsq"
   },
   "source": [
    "## Start the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za_TAI2RkPI9"
   },
   "outputs": [],
   "source": [
    "!chainlit run app.py --port=8989 --watch &>./logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZndKMZcUVNyi"
   },
   "source": [
    "## Change the Initial app screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ajK7oAJ-nx-",
    "outputId": "5338ae3a-1ea1-44ed-92dc-3dd24d52fabb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chainlit.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile chainlit.md\n",
    "\n",
    "# Welcome to File QA RAG Chatbot 🤖\n",
    "\n",
    "Please ask your question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrVhyQVirAqP",
    "outputId": "76822f6c-9d47-4c78-f7f5-8acd2f194829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainlit App: https://a4f2-34-74-217-225.ngrok-free.app\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import yaml\n",
    "\n",
    "# Terminate open tunnels if exist\n",
    "ngrok.kill()\n",
    "\n",
    "# Setting the authtoken\n",
    "# Get your authtoken from `ngrok_credentials.yml` file\n",
    "with open('ngrok_credentials.yml', 'r') as file:\n",
    "    NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN['ngrok_key'])\n",
    "\n",
    "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
    "ngrok_tunnel = ngrok.connect(8989)\n",
    "print(\"Chainlit App:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q3yFB_jsgC5"
   },
   "source": [
    "## Remove running app processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG7Abg_LrAw6"
   },
   "outputs": [],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2VA4ZtCkPNN",
    "outputId": "96042d9a-8556-4132-a400-25f8d72cd3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root           6       1  0 06:10 ?        00:00:14 /tools/node/bin/node /datalab/web/app.js\n",
      "root       23149     334  0 07:46 ?        00:00:00 /bin/bash -c ps -ef | grep app\n",
      "root       23151   23149  0 07:46 ?        00:00:00 grep app\n"
     ]
    }
   ],
   "source": [
    "!ps -ef | grep app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WxGAxGHkPLP"
   },
   "outputs": [],
   "source": [
    "!sudo kill -9 11975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlBQh5fdkPPY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
